# Analyzer GPT ğŸ¤–ğŸ“Š

An intelligent data analysis assistant powered by GPT-4 and AutoGen, designed to analyze CSV data through natural language queries. The system uses multi-agent collaboration to provide comprehensive data insights through both CLI and web interfaces.

## ğŸŒŸ Features

- **Natural Language Data Analysis**: Ask questions about your CSV data in plain English
- **Multi-Agent Architecture**: Powered by AutoGen with specialized agents for different tasks
- **Streamlit Web Interface**: User-friendly web app for interactive data analysis
- **Docker Code Execution**: Secure and isolated Python code execution environment
- **Visualization Support**: Generate charts and graphs automatically using matplotlib
- **CLI Support**: Command-line interface for programmatic usage
- **Conversation Memory**: Maintains context across multiple queries

## ğŸ—ï¸ Architecture

The system consists of two main agents:

### 1. DataAnalyzerAgent ğŸ§‘ğŸ»â€ğŸ’»
- **Role**: Expert data analyst specializing in Python and CSV data analysis
- **Responsibilities**:
  - Creates analysis plans based on user queries
  - Writes Python code to solve data problems
  - Interprets results and provides insights
  - Generates visualizations when requested

### 2. CodeExecutorAgent ğŸ¤–
- **Role**: Secure code execution in Docker containers
- **Responsibilities**:
  - Executes Python code in isolated environment
  - Installs required libraries as needed
  - Returns execution results and error messages
  - Manages file operations and outputs

## ğŸš€ Getting Started

### Prerequisites

- Python 3.12 or higher
- Docker (for code execution)
- OpenAI API key

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd "Analyzer GPT"
   ```

2. **Install dependencies**
   ```bash
   pip install -e .
   ```

3. **Set up environment variables**
   Create a `.env` file in the project root:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   ```

4. **Ensure Docker is running**
   ```bash
   docker --version
   ```

### Quick Start

#### Using the Streamlit Web Interface

1. **Run the application**
   ```bash
   streamlit run main.py
   ```

2. **Access the web interface**
   - Open your browser and navigate to the Streamlit URL (typically `http://localhost:8501`)

3. **Upload and analyze**
   - Upload a CSV file using the file uploader
   - Enter your analysis question in the chat input
   - Watch as the AI agents collaborate to analyze your data


## ğŸ› ï¸ Project Structure

```
Analyzer GPT/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ analyzer_gpt/
â”‚       â”œâ”€â”€ agents/                 # Agent implementations
â”‚       â”‚   â”œâ”€â”€ data_analyzer_agent.py
â”‚       â”‚   â””â”€â”€ code_executor_agent.py
â”‚       â”œâ”€â”€ configs/                # Configuration files
â”‚       â”‚   â””â”€â”€ constants.py
â”‚       â”œâ”€â”€ model_clients/          # OpenAI client setup
â”‚       â”‚   â””â”€â”€ openai_model_client.py
â”‚       â”œâ”€â”€ prompts/                # Agent prompts
â”‚       â”‚   â””â”€â”€ data_analyzer_prompt.py
â”‚       â”œâ”€â”€ streamlit/              # Web interface
â”‚       â”‚   â””â”€â”€ app.py
â”‚       â”œâ”€â”€ teams/                  # Team orchestration
â”‚       â”‚   â”œâ”€â”€ data_analyzer_team.py
â”‚       â”‚   â””â”€â”€ team_executor.py
â”‚       â””â”€â”€ utils/                  # Utility functions
â”‚           â”œâ”€â”€ docker_utils.py
â”‚           â””â”€â”€ logging_config.py
â”œâ”€â”€ main.py                         # Application entry point
â”œâ”€â”€ pyproject.toml                  # Project dependencies
â””â”€â”€ README.md                       # This file
```

## âš™ï¸ Configuration

The system can be configured through `src/analyzer_gpt/configs/constants.py`:

```python
MODEL = 'gpt-4o'                    # OpenAI model to use
TEXT_MENTION_TERMINATION = 'STOP'  # Termination keyword
DOCKER_WORK_DIR = 'tmp'             # Working directory for Docker
DOCKER_TIMEOUT = 120                # Docker execution timeout (seconds)
MAX_TURNS = 15                      # Maximum conversation turns
```

## ğŸ³ Docker Requirements

The application uses Docker for secure code execution. Ensure Docker is installed and running on your system. The system will automatically:

- Create Docker containers for code execution
- Install required Python packages as needed
- Manage file operations between host and container
- Clean up containers after execution

## ğŸš¨ Important Notes

- **File Handling**: Uploaded CSV files are saved as `data.csv` in the working directory
- **Output Files**: Generated visualizations are saved as `output.png`
- **Security**: Code execution is isolated in Docker containers
- **Dependencies**: Python packages are installed automatically as needed
- **Memory**: The system maintains conversation context during a session

## ğŸ”’ Security Considerations

- All code execution happens in isolated Docker containers
- Temporary files are stored in the `tmp/` directory (gitignored)
- OpenAI API keys should be kept secure and not committed to version control
- Docker containers are automatically cleaned up after execution

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request
